# AI-Augmented E-Commerce Business Intelligence System

## Project Overview

This project is an end-to-end **Business Intelligence (BI) system** for e-commerce that combines **SQL analytics**, **Python data pipelines**, **machine learning** (churn prediction), and **AI-generated executive summaries** using the Anthropic Claude API. It is designed to ingest transactional data from a PostgreSQL (or SQLite) database, compute key performance indicators, train a customer churn model, produce publication-ready visualizations, and generate stakeholder-ready narrative summaries.

The system demonstrates production-style structure: configuration-driven database connectivity, modular Python packages, reproducible dependency pinning, and clear separation between data access, analytics, visualization, modeling, and AI orchestration. It is suitable for portfolio presentation, technical interviews, and as a template for real-world BI workflows.

**Technologies used:** Python 3.x, PostgreSQL / SQLite, SQLAlchemy, pandas, scikit-learn, matplotlib, seaborn, Anthropic Claude API, PyYAML.

**Key features:** SQL-based KPI computation, automated visualizations, logistic regression churn prediction with interpretable feature importance, and AI-powered executive summary generation from analytics outputs.

---

## Business Problem

E-commerce teams need to understand **revenue trends**, **profitability by category**, **customer value**, **product contribution (Pareto)**, and **churn risk** without manually running ad-hoc queries or stitching together reports. Raw data alone does not answer “what should we do next?”—executives need synthesized insights and clear recommendations.

This system addresses that gap by:

- **Centralizing** business logic in SQL and Python so metrics are consistent and repeatable.
- **Automating** the pipeline from database to KPIs, charts, model results, and narrative summary in one run.
- **Surfacing** churn drivers via a trained model and coefficient-based feature importance.
- **Delivering** a written executive summary (generated by Claude) that interprets the numbers and suggests actions.

The result is a single command that produces dashboards, model diagnostics, and a written brief—reducing manual reporting and enabling data-driven decisions.

---

## Technical Architecture

### Technology Stack

| Layer | Technology |
|-------|------------|
| Database | PostgreSQL (primary) or SQLite |
| ORM / connection | SQLAlchemy 2.x, psycopg2-binary |
| Data processing | pandas, numpy |
| Analytics | SQL (CTEs, window functions), Python orchestration |
| Visualization | matplotlib, seaborn |
| Machine learning | scikit-learn (Logistic Regression, StandardScaler) |
| AI narrative | Anthropic Claude API (e.g. claude-sonnet-4-20250514) |
| Configuration | YAML (config/config.yaml), optional .env via python-dotenv |

### System Components

1. **DatabaseManager** (`src/database.py`) — Loads config, creates SQLAlchemy engine, executes queries and returns pandas DataFrames.
2. **BusinessAnalytics** (`src/analytics.py`) — Runs SQL-based KPI methods (monthly revenue, profit margins, CLV, Pareto, churn features) and stores results in a dictionary.
3. **BusinessVisualizations** (`src/visualizations.py`) — Consumes KPI DataFrames and generates charts (revenue, margins, CLV, Pareto, churn) into `reports/visualizations/`.
4. **ChurnPredictionModel** (`src/churn_model.py`) — Prepares features from churn_features KPI, trains logistic regression, evaluates metrics, plots feature importance and ROC curve, writes `reports/model_results.txt`.
5. **AIExecutiveSummary** (`src/ai_summary.py`) — Reads analytics results, formats them as context, calls Claude to generate an executive summary, and saves `reports/executive_summary.md`.
6. **Main pipeline** (`main.py`) — Orchestrates the above in sequence: analytics → visualizations → churn model → AI summary; closes DB connection and prints an output summary.

### Data Flow (Text Diagram)

```
┌─────────────────┐     ┌──────────────────┐     ┌─────────────────────┐
│  config/        │     │  PostgreSQL /     │     │  BusinessAnalytics  │
│  config.yaml    │────▶│  SQLite DB       │────▶│  (SQL KPIs)         │
│  .env (optional)│     │  (schema.sql)    │     │  → DataFrames       │
└─────────────────┘     └──────────────────┘     └──────────┬──────────┘
                                                              │
         ┌────────────────────────────────────────────────────┼────────────────────────────────────┐
         │                                                    │                                    │
         ▼                                                    ▼                                    ▼
┌─────────────────────┐                         ┌─────────────────────┐               ┌─────────────────────┐
│ BusinessVisualizations│                         │ ChurnPredictionModel │               │ AIExecutiveSummary  │
│ → reports/visualizations/ │                         │ → model_results.txt │               │ → executive_summary.md │
└─────────────────────┘                         │ → churn_*.png       │               └─────────────────────┘
                                                └─────────────────────┘
```

---

## Dataset Description

### Schema Overview

The system expects a relational schema with **customers**, **products**, **orders**, **order_items**, and **support_tickets**. All revenue and profit metrics use **completed orders only** unless otherwise noted.

### Table Descriptions

| Table | Description |
|-------|-------------|
| **customers** | `customer_id`, `email`, `first_name`, `last_name`, `registration_date`, `country`, `customer_segment` (High/Medium/Low Value). |
| **products** | `product_id`, `product_name`, `category` (Electronics, Apparel, Home & Garden, Sports, Books), `subcategory`, `unit_cost`, `unit_price`, `supplier_id`. |
| **orders** | `order_id`, `customer_id`, `order_date`, `order_status` (Completed, Cancelled, Returned), `shipping_cost`, `discount_amount`, `payment_method`. |
| **order_items** | `order_item_id`, `order_id`, `product_id`, `quantity`, `unit_price`, `unit_cost` (snapshot at order time). |
| **support_tickets** | `ticket_id`, `customer_id`, `ticket_date`, `issue_type`, `resolution_status` (Resolved, Unresolved). |

### Sample Data Characteristics

- **Orders:** Completed orders drive revenue, AOV, and MoM growth; cancelled/returned are excluded from KPI calculations.
- **Revenue:** Computed as `quantity * unit_price` per line item, summed per order/month; discounts applied at order level.
- **Profit:** `(unit_price - unit_cost) * quantity` per line, aggregated by category or customer.
- **Churn:** Defined as customers with at least two historical orders and no order in the last 90 days; features include tenure, recency, support tickets, and order frequency.

---

## Installation & Setup

### Prerequisites

- Python 3.9+ (recommended 3.10 or 3.11)
- PostgreSQL 12+ (or SQLite for a file-based DB)
- Anthropic API key (for AI summary generation)

### Step-by-Step Installation

```bash
# 1. Clone or copy the project and enter the project root
cd "Data Analytics Project"

# 2. Create a virtual environment (recommended)
python -m venv venv
# Windows:
venv\Scripts\activate
# macOS/Linux:
# source venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Database setup: create DB and load schema
# PostgreSQL example:
psql -U postgres -c "CREATE DATABASE ecommerce_db;"
psql -U postgres -d ecommerce_db -f sql/schema.sql
psql -U postgres -d ecommerce_db -f sql/indexes.sql

# 5. Load your data (e.g. CSV import or ETL) into customers, products, orders, order_items, support_tickets
```

### Configuration

1. **Copy the example config and edit with your values:**

   ```bash
   # config/config.yaml already exists; edit database and anthropic sections:
   # - database: host, port, name, user, password (or type: "sqlite", name: "path/to/ecommerce.db")
   # - anthropic: api_key (required for AI summary), model, max_tokens
   ```

2. **Optional: use environment variables**

   ```bash
   cp config/.env.example config/.env
   # Edit config/.env with DB_* and ANTHROPIC_API_KEY
   # Ensure your app or config loader reads .env (e.g. via python-dotenv) if desired
   ```

3. **Paths:** `config/config.yaml` may define `paths.reports`, `paths.visualizations`, `paths.data`; the default pipeline uses `reports/` and `reports/visualizations/` if not overridden.

---

## Usage

### How to Run the Pipeline

From the project root:

```bash
python main.py
```

The pipeline will:

1. Load `config/config.yaml` and connect to the database.
2. **Step 1 — Business analytics:** Compute all KPIs (monthly revenue trends, profit margins, customer lifetime value, Pareto analysis, churn features).
3. **Step 2 — Visualizations:** Generate charts into `reports/visualizations/`.
4. **Step 3 — Churn model:** Train logistic regression on churn features, evaluate, save metrics and plots to `reports/`.
5. **Step 4 — AI summary:** Send analytics context to Claude and save `reports/executive_summary.md`.
6. Print an output summary listing generated files.

### Expected Outputs

| Output | Location | Description |
|--------|----------|-------------|
| Charts | `reports/visualizations/` | Revenue, profit margins, CLV, Pareto, churn-related plots (e.g. PNG). |
| Model report | `reports/model_results.txt` | Accuracy, ROC AUC, confusion matrix, classification report, interpretation. |
| Feature importance | `reports/churn_feature_importance.png` | Bar chart of logistic regression coefficients. |
| ROC curve | `reports/churn_roc_curve.png` | ROC curve for churn classifier. |
| Executive summary | `reports/executive_summary.md` | AI-generated narrative with insights and recommendations. |

---

## Key Features

1. **SQL-based analytics (12 business queries)** — Revenue, discounts, orders, AOV, MoM growth; profit by category; customer CLV and value tier; Pareto (80/20) by product; churn features and flag. Implemented as five KPI suites with multiple SQL CTEs/queries.
2. **Python data pipeline** — Single entry point (`main.py`) runs analytics → visualizations → churn model → AI summary with shared config and DB connection.
3. **Machine learning churn prediction** — Logistic regression with standardized features and balanced class weights; train/test split, ROC AUC, confusion matrix, precision/recall.
4. **AI-powered executive summaries** — Claude interprets KPI DataFrames and produces markdown summaries with key insights and actionable recommendations.
5. **Professional visualizations** — Matplotlib/seaborn charts for monthly revenue, profit margins by category, CLV distribution, Pareto curve, and churn feature importance.

---

## Project Structure

```
Data Analytics Project/
├── main.py                 # Pipeline orchestrator
├── requirements.txt        # Pinned Python dependencies
├── README.md               # This file
├── config/
│   ├── config.yaml         # Database, Anthropic, paths
│   └── .env.example        # Example env vars (copy to .env)
├── src/
│   ├── database.py         # DatabaseManager (SQLAlchemy, execute_query)
│   ├── analytics.py        # BusinessAnalytics (SQL KPIs)
│   ├── visualizations.py   # BusinessVisualizations (charts)
│   ├── churn_model.py      # ChurnPredictionModel (sklearn)
│   └── ai_summary.py       # AIExecutiveSummary (Claude API)
├── sql/
│   ├── schema.sql          # Table definitions (customers, products, orders, order_items, support_tickets)
│   └── indexes.sql         # Indexes for common lookups
├── reports/
│   ├── model_results.txt   # Churn model metrics and interpretation
│   ├── executive_summary.md # AI-generated summary (after run)
│   └── visualizations/     # Generated charts (PNG)
└── data/                   # Optional raw/processed data (see config paths)
```

---

## KPIs Calculated

- **Monthly revenue trends** — Last 12 months: revenue, discounts, order/customer counts, net revenue, AOV, month-over-month growth %.
- **Profit margins by category** — Total revenue, cost, gross profit, and profit margin % per product category; sorted by gross profit.
- **Customer lifetime value** — Per-customer: total orders, revenue, profit, tenure (days), value tier (Platinum/Gold/Silver/Bronze), CLV score.
- **Pareto analysis** — Per-product revenue, revenue share %, cumulative share %, and segment (Top 20% vs Bottom 80%).
- **Churn prediction** — Features: total_orders, total_spent, avg_order_value, tenure, days_since_last_order, recent/unresolved support tickets, avg_days_per_order, ticket_to_order_ratio; target `is_churned` (no order in 90 days with 2+ prior orders). Used to train the logistic regression model and feed visualizations and AI summary.

---

## AI Integration

### How Claude API Is Used

The **AIExecutiveSummary** class in `src/ai_summary.py`:

1. Reads `config/config.yaml` for `anthropic.api_key`, `model`, and `max_tokens`.
2. Receives a dictionary of analytics DataFrames (monthly_revenue, profit_margins, clv, pareto, churn_features).
3. Converts each DataFrame to a concise markdown section (e.g. key columns, sample rows, summary stats).
4. Builds a single context string and sends it to the Anthropic API with a system/user prompt asking for an executive summary with insights and recommendations.
5. Writes the model’s response to `reports/executive_summary.md`.

### Example Prompt Structure

- **System prompt:** Instructs the model to act as a business analyst and produce a clear, actionable executive summary.
- **User prompt:** Contains the formatted analytics sections (markdown) and asks for key findings, trends, and recommendations.

### Output Format

- Markdown document with headings, bullet points, and optional tables.
- Sections typically include: overview, revenue/margin insights, customer value and churn, Pareto/product focus, and recommended actions.

---

## Machine Learning Model

- **Model type:** Logistic Regression (scikit-learn), with `class_weight='balanced'` and `max_iter=1000`.
- **Features:** total_orders, total_spent, avg_order_value, customer_tenure_days, days_since_last_order, recent_support_tickets, unresolved_tickets, avg_days_per_order, ticket_to_order_ratio (standardized via StandardScaler).
- **Target:** Binary `is_churned` (1 = churned, 0 = active).
- **Performance metrics:** Train/test accuracy, ROC AUC, confusion matrix, precision/recall/F1 per class (e.g. Active, Churned).
- **Feature importance:** Coefficients of the logistic regression; positive coefficient → higher predicted churn probability, negative → lower. Visualized in `churn_feature_importance.png`.
- **Business interpretation:** High positive weight on e.g. `days_since_last_order` or `unresolved_tickets` suggests those are churn drivers; the report in `model_results.txt` explains how to read coefficients and balance precision/recall for business cost.

---

## Limitations

- **Database:** Built and tested with PostgreSQL and SQLite; dialect-specific SQL (e.g. `date_trunc`, `INTERVAL`) may need adjustment for other engines.
- **Churn definition:** Fixed rule (no order in 90 days + 2+ prior orders); not configurable via config.
- **AI summary:** Depends on Anthropic API availability and rate limits; requires valid API key and network access.
- **Paths:** Default outputs are under `reports/`; optional `paths` in config may not be wired in every script yet.
- **Data volume:** No explicit pagination or chunking; very large tables may require query or memory optimizations.

---

## Future Improvements

- **Configurable churn window:** Make “90 days” and minimum order count configurable in YAML.
- **Additional models:** E.g. XGBoost or Random Forest for churn with optional feature importance comparison.
- **Scheduling:** Run pipeline via cron or task scheduler; optional email/Slack delivery of summary and key charts.
- **Paths from config:** Consistently read `paths.reports`, `paths.visualizations`, `paths.data` from config in all modules.
- **.env integration:** Load `config/.env` in main or database/AI modules and override config values for DB and API key.
- **Tests:** Unit tests for analytics SQL (e.g. mock DB), churn model train/eval, and AI summary formatting.
- **Docker:** Dockerfile and docker-compose for DB + app for one-command setup.

---

## AI Usage Transparency

This section explains how AI tools were used in the project and how they support (rather than replace) technical and product judgment.

### How AI Tools Were Used

- **Claude (Anthropic):** Used at **runtime** as part of the product: the pipeline sends analytics context to the Claude API and receives an executive summary. The prompts and integration code were designed by the author; the model generates the narrative text only.
- **Cursor / AI-assisted coding:** Used during **development** for suggestions, boilerplate, and refactors (e.g. docstrings, type hints, config structure). Architecture decisions (e.g. which KPIs, schema design, pipeline order, model choice) were made by the author.

### What AI Did vs. Human Decisions

- **AI did:** Generate natural-language executive summaries from tabular context; suggest code snippets and edits during implementation.
- **Human did:** Define business logic (which KPIs, churn definition, value tiers); design the schema and pipeline; choose technologies and dependencies; write and refine SQL and Python structure; validate outputs and interpret model results; draft and approve documentation (including this README).

### Validation Process

- **Analytics:** SQL was run against a sample or real DB; outputs (DataFrames, row counts) were spot-checked.
- **Churn model:** Metrics (accuracy, ROC AUC, confusion matrix) and feature coefficients were reviewed for sanity and business meaning.
- **AI summary:** Sample runs were read to ensure the summary reflects the data and does not hallucinate numbers; prompts were tuned as needed.

### Why This Demonstrates Skill, Not Shortcuts

Using an LLM for narrative generation and an IDE with AI assistance is analogous to using a calculator or a library: it augments productivity but does not substitute for understanding. This project demonstrates:

- **System design:** Orchestration of DB, analytics, ML, and AI in one pipeline with clear interfaces.
- **Data engineering:** Schema design, SQL for KPIs, and pandas for passing data between stages.
- **ML literacy:** Choice of model, evaluation metrics, and interpretation of coefficients.
- **Integration:** Config-driven setup, API usage (Anthropic), and file-based outputs.
- **Documentation:** Clear README and config examples for recruiters and future maintainers.

The value is in the **end-to-end BI system** and the **decisions** behind it; AI tools were used to speed up implementation and to deliver a user-facing feature (executive summary), not to replace analysis or design.

---

## Contact

**bernhardcolin8@gmail.com**

---

## License

MIT License
